{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "This is heavily influenced from https://github.com/pytorch/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10\n",
    "\n",
    "In thins notebook you need to put what you have learned into practice, and create your own convolutional classifier for the CIFAR-10 dataset.\n",
    "\n",
    "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "![cifar10](../static_files/cifar10.png)\n",
    "\n",
    "\n",
    "In order to train a classifier the following steps needs to be performed:\n",
    "\n",
    "1. Load and normalizing the CIFAR10 training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data\n",
    "\n",
    "We will help you along the way.\n",
    "We indicate the places you need to modify the code with `# Your code here!`.\n",
    "It is however a good idea to read the entire assignment before you begin coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and normalizing CIFAR10\n",
    "\n",
    "Using ``torchvision``, it’s extremely easy to load CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalized range [-1, 1]\n",
    "\n",
    "**NB** Modify the code below to only use a small part of the dataset if your computer is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                          (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "print()\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "used_categories = range(len(classes))\n",
    "\n",
    "## USE CODE BELOW IF YOUR COMPUTER IS TOO SLOW\n",
    "#reduce_dataset = True\n",
    "reduce_dataset = False\n",
    "if reduce_dataset:\n",
    "    used_categories = (3, 5) # cats and dogs\n",
    "    \n",
    "\n",
    "    classes = [classes[i] for i in used_categories]\n",
    "    new_train_data = []\n",
    "    new_train_labels = []\n",
    "\n",
    "    new_test_data = []\n",
    "    new_test_labels = []\n",
    "    for i, t in enumerate(used_categories):\n",
    "        new_train_data.append(trainset.train_data[np.where(np.array(trainset.train_labels) == t)])\n",
    "        new_train_labels += [i for _ in range(new_train_data[-1].shape[0])]\n",
    "\n",
    "        new_test_data.append(testset.test_data[np.where(np.array(testset.test_labels) == t)])\n",
    "        new_test_labels += [i for _ in range(new_test_data[-1].shape[0])]\n",
    "\n",
    "    new_train_data = np.concatenate(new_train_data, 0)\n",
    "    trainset.train_data = new_train_data\n",
    "    trainset.train_labels = new_train_labels\n",
    "\n",
    "    new_test_data = np.concatenate(new_test_data, 0)\n",
    "    testset.test_data = new_test_data\n",
    "    testset.test_labels = new_test_labels\n",
    "\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "train_data_iter = iter(trainloader)\n",
    "test_data_iter = iter(testloader)\n",
    "print('used classes:', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data\")\n",
    "print(trainset.train_data.shape)\n",
    "print(len(trainset.train_labels))\n",
    "print()\n",
    "\n",
    "print(\"Test data\")\n",
    "print(testset.test_data.shape)\n",
    "print(len(testset.test_labels))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show some of the training images, for fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple time to see more samples\n",
    "\n",
    "def imshow(img):\n",
    "    \"\"\" show an image \"\"\"\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "images, labels = train_data_iter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a Convolutional Neural Network\n",
    "\n",
    "**Assignment 1:** Define a convolutional neural network. \n",
    "You may use the code from previous notebooks.\n",
    "We suggest that you start with a small network, and make sure that everything is working.\n",
    "Once you can train successfully come back and improve the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels, height, width = trainset.train_data.shape[-1],trainset.train_data.shape[1],trainset.train_data.shape[2]\n",
    "conv1_kernel_num = 16\n",
    "conv1_kernel_size = 5\n",
    "conv1_kernel_stride = 1\n",
    "\n",
    "conv1_pool_size = 2\n",
    "conv1_pool_stride = 2\n",
    "\n",
    "num_hidden = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NetNotFull(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NetNotFull, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=channels,out_channels=conv1_kernel_num,kernel_size=conv1_kernel_size,stride=conv1_kernel_stride),\n",
    "                                  nn.Tanh(),\n",
    "                                  nn.MaxPool2d(kernel_size=conv1_pool_size,stride=conv1_pool_stride))\n",
    "        self.num_features = conv1_kernel_num*14*14\n",
    "        \n",
    "        #self.fc1 = nn.Sequential(nn.Linear(in_features=self.num_features,out_features=num_hidden, bias=True),\n",
    "                                #nn.Tanh())\n",
    "        self.out = nn.Linear(in_features=self.num_features,out_features=num_classes,bias=False)\n",
    "        #self.out = nn.Linear(in_features=num_hidden,out_features=num_classes,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        #print(\"*******\",x.size())\n",
    "        x = x.view(-1,self.num_features)\n",
    "        #x = self.fc1(x)\n",
    "        return F.softmax(self.out(x),dim=1)\n",
    "    \n",
    "\n",
    "netnotfull = NetNotFull(len(used_categories)).cuda()\n",
    "print(netnotfull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=channels,out_channels=conv1_kernel_num,kernel_size=conv1_kernel_size,stride=conv1_kernel_stride),\n",
    "                                  nn.Tanh(),\n",
    "                                  nn.MaxPool2d(kernel_size=conv1_pool_size,stride=conv1_pool_stride))\n",
    "        self.num_features = conv1_kernel_num*14*14\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(in_features=self.num_features,out_features=num_hidden, bias=True),\n",
    "                                nn.Tanh())\n",
    "        self.out = nn.Linear(in_features=num_hidden,out_features=num_classes,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        #print(\"*******\",x.size())\n",
    "        x = x.view(-1,self.num_features)\n",
    "        x = self.fc1(x)\n",
    "        return F.softmax(self.out(x),dim=1)\n",
    "    \n",
    "\n",
    "net = Net(len(used_categories)).cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Loss function and optimizer\n",
    "\n",
    "**Assignment 2:** Implement the criterion and optimizer. \n",
    "We suggest Classification Cross-Entropy loss and SGD with momentum.\n",
    "You might need to experiment a bit with the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Your code here!\n",
    "optimizer = optim.SGD(params=net.parameters(),lr=0.001,momentum=0.9) # Your code here!\n",
    "#SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the network\n",
    "\n",
    "**Assignment 3:** Finish the training loop below. \n",
    "Start by using a small number of epochs (e.g. 3).\n",
    "Even with a low number of epochs you should be able to see results that are better than chance.\n",
    "When everything is working increase the number of epochs to find out how good your network really is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 10  # Your code here!\n",
    "loss = []\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        batch_size = inputs.shape[0]\n",
    "        #print(\"----\",inputs.shape)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        output = net(inputs)\n",
    "        #print(output)\n",
    "        loss = criterion(output,labels)\n",
    "        #print(type(batch_loss))\n",
    "        #running_loss+=batch_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the network on the test data\n",
    "\n",
    "Now we need to check if the network has learnt anything at all.\n",
    "We will check this by predicting the class label that the neural network outputs, and checking it against the ground truth.\n",
    "If the prediction is correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display an image from the test set to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = test_data_iter.next()\n",
    "#print(type(images))\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "plt.show()\n",
    "\n",
    "print('GroundTruth:  ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "#outputs=net(images)\n",
    "outputs = net(Variable(images).cuda())\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print('Predicted:    ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how the network performs on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images).cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the {} test images: {:4.2f} %'.format(\n",
    "    testset.test_data.shape[0], 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully the network is better than chance, which is $\\frac{1}{\\text{number of classes}}$ accuracy (randomly picking\n",
    "a class).\n",
    "\n",
    "\n",
    "We can also examine which class the network found the most difficult (makes more sense if you have many clases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_total = list(0. for i in range(len(classes)))\n",
    "class_correct = list(0. for i in range(len(classes)))\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images).cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted.cpu() == labels).squeeze()\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        label = labels[i]\n",
    "        #print(type(c[i]))\n",
    "        class_correct[label] += c[i]#.numpy()\n",
    "        class_total[label] += 1\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    print('Accuracy of {:5s} : {:5.2f} %'.format(\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 4:** \n",
    "1. Go back and improve performance of the network. \n",
    " * If you are using all 10 classes you should get a test accuracy above 55%, but see how much further you can get it!\n",
    " * If you are using only 2 classes (e.g. cat and dog) you should get a test accuracy above 60%, but see how much further you can get it!\n",
    "\n",
    "2. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
    "Did anything surprise you during the exercise?\n",
    "\n",
    "3. Write down key lessons/insights you got (if any) during this exercise.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "<font color='red'>with only ten classes, I got the accuracy as 55%, setting the number of epochs equal to 5.\n",
    "With only two classes, the accuracy is around 70%, setting the number of epochs equal to 5. </font>\n",
    "\n",
    "\n",
    "<font color='red'>I tried the following experiment:</font>\n",
    "    \n",
    "    1.Adam optimizer, the performance significantly decreases, in particular for the 10-classes classifier, it ended up with the accuracy around 15%.\n",
    "    \n",
    "    2.increase the number of iterations to 10 for 10-classes classifier, the output is 61%, improved by 6% w.r.t 55%\n",
    "    \n",
    "    3. modify the learning rate from 0.001 to 0.005\n",
    "    \n",
    "    4. changed the activation function into Tanh, with 10 epochs, the accuracy is 61%, the same as with ReLu.\n",
    "    \n",
    "When deal with relative big and complicated dataset, the choice of optimizer may become critical, which is different when I tried the MNIST dataset.\n",
    "\n",
    "Note that I train the classifier on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                          (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "#print()\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer','dog', 'frog', 'horse', 'ship', 'truck')\n",
    "#print(classes)\n",
    "used_categories2 = range(len(classes))\n",
    "#print(\"used\",used_categories)\n",
    "## USE CODE BELOW IF YOUR COMPUTER IS TOO SLOW\n",
    "reduce_dataset = True\n",
    "if reduce_dataset:\n",
    "    used_categories = (3, 5) # cats and dogs\n",
    "\n",
    "    classes = [classes[i] for i in used_categories2]\n",
    "    new_train_data = []\n",
    "    new_train_labels = []\n",
    "\n",
    "    new_test_data = []\n",
    "    new_test_labels = []\n",
    "    for i, t in enumerate(used_categories2):\n",
    "        new_train_data.append(trainset.train_data[np.where(np.array(trainset.train_labels) == t)])\n",
    "        new_train_labels += [i for _ in range(new_train_data[-1].shape[0])]\n",
    "\n",
    "        new_test_data.append(testset.test_data[np.where(np.array(testset.test_labels) == t)])\n",
    "        new_test_labels += [i for _ in range(new_test_data[-1].shape[0])]\n",
    "\n",
    "    new_train_data = np.concatenate(new_train_data, 0)\n",
    "    trainset.train_data = new_train_data\n",
    "    trainset.train_labels = new_train_labels\n",
    "\n",
    "    new_test_data = np.concatenate(new_test_data, 0)\n",
    "    testset.test_data = new_test_data\n",
    "    testset.test_labels = new_test_labels\n",
    "\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "train_data_iter = iter(trainloader)\n",
    "test_data_iter = iter(testloader)\n",
    "print('used classes:', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=channels,out_channels=conv1_kernel_num,kernel_size=conv1_kernel_size,stride=conv1_kernel_stride),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(kernel_size=conv1_pool_size,stride=conv1_pool_stride))\n",
    "        self.num_features = conv1_kernel_num*14*14\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(in_features=self.num_features,out_features=num_hidden, bias=True),\n",
    "                                nn.ReLU())\n",
    "        self.out = nn.Linear(in_features=num_hidden,out_features=num_classes,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = x.view(-1,self.num_features)\n",
    "        x = self.fc1(x)\n",
    "        return F.softmax(self.out(x),dim=1)\n",
    "    \n",
    "\n",
    "net = Net(len(used_categories2)).cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Your code here!\n",
    "optimizer = optim.Adam(params=net.parameters(),lr=0.1) # Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with all the classes\n",
    "num_epoch = 5  # Your code here!\n",
    "#loss = []\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "        # zero the parameter gradients\n",
    "        output = net(inputs)\n",
    "        loss = criterion(output,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        #print(loss.data)\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images).cuda())\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the {} test images: {:4.2f} %'.format(\n",
    "    testset.test_data.shape[0], 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GPU\n",
    "\n",
    "**Optional Assignment:**\n",
    "If you have a GPU we suggest that you try and rewrite the code above to run on the GPU\n",
    "___\n",
    "\n",
    "Just like how you transfer a Tensor on to the GPU, you transfer the neural net onto the GPU.\n",
    "This will recursively go over all modules and convert their parameters and buffers to CUDA tensors:\n",
    "\n",
    "```\n",
    "    net.cuda()\n",
    "```\n",
    "\n",
    "Remember that you will have to send the inputs and targets at every step to the GPU too:\n",
    "\n",
    "```\n",
    "    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "```\n",
    "\n",
    "Why dont I notice MASSIVE speedup compared to CPU? \n",
    "Because your network is realllly small.\n",
    "\n",
    "**Exercise:** Try increasing the width of your network (argument 2 of\n",
    "the first ``nn.Conv2d``, and argument 1 of the second ``nn.Conv2d`` –\n",
    "they need to be the same number), see what kind of speedup you get.\n",
    "\n",
    "**Goals achieved**:\n",
    "\n",
    "- Understanding PyTorch's Tensor library and neural networks at a high level.\n",
    "- Train a small neural network to classify images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michael Nielsen book exercise of own choice\n",
    "\n",
    "**Assignment 5:** Pick an exercise of own choice from [Michael Nielsens book](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
